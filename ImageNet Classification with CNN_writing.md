# ImageNet Classification with Deep Convolutional Neural Networks

Alex Krizhevsky, Ilya Sutskever, Geoffrey E.Hinton

## Abstract

  1000개의 다른 클래스를 가진 ImageNet LSVRC-2010의 1.2백만개의 고해상도의 이미지를 CNN으로 학습하였고, test data에서 top-1 error rates는 37.5%, top-5 error rates는 17.0%으로 이전의 결과보다 상당히 좋은 결과를 냈다. 6천만개의 파라미터와 max-pooling 층에 선행되는 5개의 콘볼루션 층과  65만개의 뉴런으로 만들어진  신경망은 1000가지 방법의 softmax와 함께 3개의 완전히 연결된 층을 구성한다. 

  학습을 더 빠르게 하기 위해서, non-saturating neuron들과 효율적인 GPU를 사용하며 fully-connected layer들 안에서의 과적합을 줄이기 위해 "dropout"이라는 정규화 방법을 도입하였다. 또한 ILSVRC-2012 대회에서도 이 모델의 변형을 사용하였고, 좋은 결과를 이루었다.

## 1. Introduction

 사물 인식의 현 접근은 머신 러닝을 필수로 사용한다. 이들의 수행 능력을 높이기 위해 **더 많은 데이터 셋을 수집하거나** **강력한 모델**, 그리고 과적합을 방지하기 위한 **더 좋은 기술**들을 사용한다.

  적은 데이터 셋의 단점을 널리 인식되었으나, 최근 들어서 수백만장의 labeled된 데이터셋을 모을 수 있게 됨으로써 극복할 수 있게 되었다.

  수백만 장의 이미지으로부터 수천 가지의 사물에 대해 배우기 위해, 모델이 더 많은 learning 능력을 가져야 한다. 그러나 사물 인식 문제의 방대한 복잡성은 ImageNet과 같은 큰 데이터 셋임으로도 이 문제를 해결할 수 없어서, 모델은 미지의 데이터를 예측하기 위해 많은 사전 지식 또한 습득해야 한다.

  CNN은 depth와 breadth 파라미터를 조정함으로써 조율할 수 있고, 이미지의 특성에 대해 강력하고 대부분 정확한 가정을 할 수 있다.

  따라서 비슷한 크기의 층을 가진 순 방향 신경망과 비교했을 때, CNN은 비교적 적은 연결과 파라미터를 가지고 있고 학습하기 쉽다는 장점이 있는 반면, CNN의 이론적으로 최상의 수행 능력은 약간 좋지 않다는 단점이 있다.

  CNN의 매력적인 속성과 그것의 local architecture의 상대적 효율성에도 불구하고, 여전히 고해상도의 이미지의 규모에 적용하기에 엄청난 비용이 발생한다. 다행이게도 고도로 최적화된 2D 컨볼루션 구현과 결합된 현재의 GPUs는 큰 규모의 CNN이 학습하기 용이하게 하고, ImageNet과 같은 최근의 데이터 셋은 극심한 과적합 없이 이러한 모델을 학습하기 충분한 레이블이 지정된 표본을 가지고 있다.

  본 논문의 구체적인 기여는 다음과 같다.

- ILSVRC-2010과 2012 대회에 사용된 ImageNet 데이터를 큰 규모의 CNN에 학습시켰으며 이 데이터 셋에 대하여 지금까지 어느 모델보다도 가장 좋은 결과를 냈다.
- 2D 컨볼루션에 고도로 최적화된 GPU과 학습된 CNN에 내재된 또 다른 작동에 대해 공개적으로 사용할 수 있도록 서술하였다.
- 본 모델은 수행 능력을 향상시키고 수행 시간을 줄여주는 새롭고 평범하지 않은  feature을 포함하고 있다. (Section 3)
- 과적합을 막을 수 있는 몇몇의 효과적인 테크닉을 서술하였다. (Section 4)
- 어떠한 컨볼루션 층(모델의 파라미터의 1% 미만을 포함하는 각 층)을 제거하는 것은 저조한 성능을 보여주는 것을 확인하였다.

## 2. The Data set